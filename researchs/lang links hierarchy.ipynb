{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notebook to study the best way to architect a db to store wikipedia articles multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py2neo #Libs to connect to neo4j\n",
    "\n",
    "def connectDb():\n",
    "    \"\"\"Function to connect to neo4j database.\"\"\"\n",
    "\n",
    "    py2neo.authenticate(\"localhost:7474\", \"neo4j\", \"lucas\")\n",
    "    dbConnection = py2neo.Graph(\"http://localhost:7474/db/data/\")\n",
    "\n",
    "    return dbConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wikipydia import wikipedia, wikilinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#art = wikipedia.get_article_by_href(\"C%2b%2b\")\n",
    "#wikilinks.get_article_links_score(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"Lu'cas\".replace(\"'\", \"\\\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neo4j.v1 import GraphDatabase, basic_auth\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "class Wiki4Neo():\n",
    "    def __init__(self, user=\"neo4j\", password=\"lucas\"):\n",
    "        \"\"\"Connect to neo4j db\"\"\"\n",
    "        self.driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=basic_auth(user, password))\n",
    "        \n",
    "        #session = driver.session()\n",
    "        #session.run(\"CREATE (a:Person {name: {name}, title: {title}})\",{\"name\": \"Arthur\", \"title\": \"King\"})\n",
    "        #result = session.run(\"MATCH (a:Person) WHERE a.name = {name} RETURN a.name AS name, a.title AS title\",{\"name\": \"Arthur\"})\n",
    "        #for record in result:\n",
    "        #    print(\"%s %s\" % (record[\"title\"], record[\"name\"]))\n",
    "        #session.close()\n",
    "\n",
    "    def save_article(self, article, links, url, lang):\n",
    "        #Ensure to escape quotes from the title and url before query\n",
    "        #\"Lu'cas\".replace(\"'\", \"\\\\'\")\n",
    "        #pageInfo.title = pageInfo.title.replace(/['\\\\]/g, \"\\\\$&\");\n",
    "        #pageInfo.url = pageInfo.url.replace(/['\\\\]/g, \"\\\\$&\");\n",
    "\n",
    "        #Construct query\n",
    "        neo_query = \"\\n\".join([\n",
    "            'MERGE (article:WikiArticle{{title:\"{}\", pageId:{}}})'.format(article.title(), article.page_id()), \n",
    "            'ON CREATE SET article.lang = \"{}\"'.format(lang), \n",
    "            'MERGE (articleUrl:Wikiurl{{url_lang:\"{}\"}})'.format(url + \"_\" + lang),\n",
    "            'ON CREATE SET articleUrl.url = \"{}\", articleUrl.lang = \"{}\"'.format(url, lang),\n",
    "            'SET articleUrl.articleId = {}'.format(article.page_id()),\n",
    "            \"CREATE UNIQUE (articleUrl)-[:RedirectsTo]->(article)\"\n",
    "        ])\n",
    "        \n",
    "        #Create queries for links to the article\n",
    "        for i, (link_href, link_score) in enumerate(links):\n",
    "            #Ensure to escape quotes from the link before query\n",
    "            #var link = pageInfo.links[i].replace(/['\\\\]/g, \"\\\\$&\");\n",
    "            link_href = unquote(link_href) #Remove url quotes from the href\n",
    "\n",
    "            neo_query += \"\\n\\n\" + \"\\n\".join([\n",
    "                'MERGE (articleLink{}:Wikiurl{{url_lang: \"{}\"}})'.format(i, link_href+\"_\"+lang),\n",
    "                'ON CREATE SET articleLink{}.url = \"{}\", articleLink{}.lang = \"{}\"'.format(i,link_href,i,lang),\n",
    "                'CREATE UNIQUE (article)-[:LinksTo{{score:{}}}]->(articleLink{})'.format(link_score, i)\n",
    "            ])\n",
    "            \n",
    "        #Create query to create direct connection between articles\n",
    "        if False:\n",
    "            neo_query += \"\\n\"\n",
    "\n",
    "            #Add in/out ConnectsTo relation to this article \n",
    "            neo_query += \"\\n\".join([\n",
    "                \"WITH article MATCH (article)-[:LinksTo]->(:Wikiurl)-[:RedirectsTo]->(targetArticle:Article)\",\n",
    "                \"CREATE UNIQUE (article)-[:ConnectsTo]->(targetArticle)\",\n",
    "                \"WITH article MATCH (targetArticle:Article)-[:LinksTo]->(:Wikiurl)-[:RedirectsTo]->(article)\", \n",
    "                \" CREATE UNIQUE (targetArticle)-[:ConnectsTo]->(article)\"\n",
    "            ])\n",
    "            \n",
    "        neo_query += \"\\nRETURN id(article) as article_id\"\n",
    "\n",
    "        session = self.driver.session()\n",
    "        results = session.run(neo_query)\n",
    "        session.close()\n",
    "        \n",
    "        for record in results:\n",
    "            return record[\"article_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w4n = Wiki4Neo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_article_and_links_scores_by_href(href, lang):\n",
    "    art = wikipedia.get_article_by_href(href, lang)\n",
    "    links = wikilinks.get_article_links_score(art)\n",
    "    art_id = w4n.save_article(art, links, href, lang)\n",
    "    print(art_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_article_and_links_scores_by_href(\"c%2b%2b\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from urllib.parse import quote\n",
    "#for lang, article in wikipedia.get_article_langlinks(\"MQTT\"):\n",
    "    #article = quote(article) #quote article title to use as href\n",
    "    #save_article_and_links_scores_by_href(article, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_download_pagedata_by_href(href, lang):\n",
    "    \"\"\"\n",
    "    Get data from database, downloading missing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #1. Get the wikiarticle data from the db by the href \n",
    "    neo_query = \"\\n\".join([\n",
    "        'MATCH (n:Wikiurl{{url:\"{}\", lang:\"{}\"}})-[:RedirectsTo]->(a:WikiArticle)',\n",
    "        'RETURN a.title as title, a.pageId as pageid, id(a) as id'\n",
    "    ]).format(href, lang)\n",
    "    \n",
    "    session = w4n.driver.session()\n",
    "    results = session.run(neo_query)\n",
    "    session.close()\n",
    "    \n",
    "    #print(\"Time of the 1st try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    for r in results:\n",
    "        return r['title'], r['pageid'], r['id']\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #2. If the href doesn't return an article, download this href\n",
    "    #Try to get the article\n",
    "    #If suceed, save the href and try to return the article and create a reference from the href\n",
    "    art = wikipedia.get_article_by_href(href, lang)\n",
    "    \n",
    "    neo_query = \"\\n\".join([\n",
    "        'MERGE (u:Wikiurl{{url_lang:\"{}\"}})'.format(href + \"_\" + lang),\n",
    "        'ON CREATE SET u.url = \"{}\", u.lang = \"{}\", u.articleId = {}'.format(href, lang, art.page_id()),\n",
    "        'WITH u MATCH (a:WikiArticle{{pageId:{}}})'.format(art.page_id()),\n",
    "        'CREATE UNIQUE (u)-[:RedirectsTo]->(a)',\n",
    "        'RETURN a.title as title, a.pageId as pageid, id(a) as id'\n",
    "    ])\n",
    "    \n",
    "    session = w4n.driver.session()\n",
    "    results = session.run(neo_query)\n",
    "    session.close()\n",
    "    #print(\"Time of the 2nd try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    for r in results:\n",
    "        return r['title'], r['pageid'], r['id']\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #3. If nothing has been returned till now, so the article doesn't exists, lets register it\n",
    "    links = wikilinks.get_article_links_score(art)\n",
    "    new_art_id = w4n.save_article(art, links, href, lang)\n",
    "    #print(\"Time of the 3rd try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    return art.title(), art.page_id(), new_art_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_or_download_pagedata_by_href(\"c%2b%2b\", \"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "def download_and_save_global_articles(href_en):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #global_articles = list()\n",
    "\n",
    "    #href_en = \"MQTT\"\n",
    "    \n",
    "    #1. Download english article and get links scores\n",
    "    #art_en = wikipedia.get_article_by_href(href_en, \"en\")\n",
    "    #links_scores = wikilinks.get_article_links_score(art_en)\n",
    "    #global_articles.append((art_en, links_scores, href_en, \"en\"))\n",
    "    #global_article_title = art_en.title()\n",
    "    #global_article_pageid = art_en.page_id()\n",
    "    \n",
    "    #1.1 Get or download english article data\n",
    "    print(\"Getting english article data...\")\n",
    "    art_en_title, art_en_page_id, art_en_id = get_or_download_pagedata_by_href(href_en, \"en\")\n",
    "    \n",
    "    global_article_title = art_en_title\n",
    "    global_article_pageid = art_en_page_id\n",
    "    \n",
    "    #2. Get global links by english article title\n",
    "    print(\"Getting global links...\")\n",
    "    #global_links = wikipedia.get_article_langlinks(art_en.title())\n",
    "    global_links = wikipedia.get_article_langlinks(art_en_title)\n",
    "    \n",
    "\n",
    "    #3. Download articles and generate links scores fot the global links\n",
    "    #for art_lang, art_title in global_links:\n",
    "        #art_href = quote(art_title)\n",
    "        #art = wikipedia.get_article_by_href(art_href, art_lang)\n",
    "        #art_links_scores = wikilinks.get_article_links_score(art)\n",
    "        #global_articles.append((art, art_links_scores, art_href, art_lang))\n",
    "        \n",
    "    global_pageids = list()\n",
    "    print(\"Getting global links articles data...\")\n",
    "    #3.1 Register hrefs from the global links and get their register ids\n",
    "    for i, (art_lang, art_title) in enumerate(global_links):\n",
    "        print(\"{}/{}\".format(i+1, len(global_links)))\n",
    "        art_href = quote(art_title) #get href by quoting title\n",
    "        art_title, art_page_id, art_id = get_or_download_pagedata_by_href(art_href, art_lang)\n",
    "        global_pageids.append(art_id)\n",
    "        \n",
    "    print(global_pageids)\n",
    "\n",
    "    #4. Save them into the database and get their ids\n",
    "    #global_pageids = list()\n",
    "    #for art in global_articles:\n",
    "        #new_page_ids = w4n.save_article(art[0], art[1], art[2], art[3])\n",
    "        #global_pageids.append(new_page_ids)\n",
    "    \n",
    "    \n",
    "    #5. Points every article to the global article of its kind\n",
    "    print(\"Saving global articles...\")\n",
    "    neo_query = \"\\n\".join([\n",
    "        'MERGE (art:GlobalArticle{{title:\"{}\", en_page_id:{}}})'.format(global_article_title, global_article_pageid),\n",
    "        \"WITH art MATCH (target_art:WikiArticle) WHERE ID(target_art) IN {}\".format(global_pageids),\n",
    "        \"CREATE UNIQUE (art)<-[:DerivesFrom]-(target_art)\"\n",
    "    ])    \n",
    "\n",
    "    session = w4n.driver.session()\n",
    "    session.run(neo_query)\n",
    "    session.close()\n",
    "    print(\"Done ({} seconds.)\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "#start_time = time.time()\n",
    "#get_or_download_pagedata_by_href(\"MQTT\", \"pt\")\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting english article data...\n",
      "Getting global links...\n",
      "Getting global links articles data...\n",
      "1/9\n",
      "2/9\n",
      "3/9\n",
      "4/9\n",
      "5/9\n",
      "6/9\n",
      "7/9\n",
      "8/9\n",
      "9/9\n",
      "[37, 65, 89, 97, 107, 115, 120, 127, 134]\n",
      "Saving global articles...\n",
      "Done\n",
      "12.043689012527466\n"
     ]
    }
   ],
   "source": [
    "download_and_save_global_articles(\"MQTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wikipedia.get_article_langlinks(\"Artificial_neural_network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
