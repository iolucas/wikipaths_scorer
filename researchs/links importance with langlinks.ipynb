{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of calculation of links importance using other languages wiki articles together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wikipydia import wikipedia, wikilinks, url\n",
    "from wikipydia.url import QuotedURL, UnquotedURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for ll in wikipedia.get_article_langlinks(UnquotedURL(\"C++\")):\n",
    "    #print(ll[2].quoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neo4j.v1 import GraphDatabase, basic_auth\n",
    "\n",
    "#from urllib.parse import unquote\n",
    "\n",
    "class Wiki4Neo():\n",
    "    def __init__(self, user=\"neo4j\", password=\"lucas\"):\n",
    "        \"\"\"Connect to neo4j db\"\"\"\n",
    "        self.driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=basic_auth(user, password))\n",
    "        \n",
    "        #session = driver.session()\n",
    "        #session.run(\"CREATE (a:Person {name: {name}, title: {title}})\",{\"name\": \"Arthur\", \"title\": \"King\"})\n",
    "        #result = session.run(\"MATCH (a:Person) WHERE a.name = {name} RETURN a.name AS name, a.title AS title\",{\"name\": \"Arthur\"})\n",
    "        #for record in result:\n",
    "        #    print(\"%s %s\" % (record[\"title\"], record[\"name\"]))\n",
    "        #session.close()\n",
    "\n",
    "    def save_article(self, article, links, url, lang):\n",
    "        #Ensure to escape quotes from the title and url before query\n",
    "        #\"Lu'cas\".replace(\"'\", \"\\\\'\")\n",
    "        #pageInfo.title = pageInfo.title.replace(/['\\\\]/g, \"\\\\$&\");\n",
    "        #pageInfo.url = pageInfo.url.replace(/['\\\\]/g, \"\\\\$&\");\n",
    "\n",
    "        article_title = escp(article.title())\n",
    "        url = escp(url)\n",
    "\n",
    "        #Construct query\n",
    "        neo_query = \"\\n\".join([\n",
    "            'MERGE (article:WikiArticle{{title:\"{}\", pageId:{}}})'.format(article_title, article.page_id()), \n",
    "            'ON CREATE SET article.lang = \"{}\"'.format(lang), \n",
    "            'MERGE (articleUrl:Wikiurl{{url_lang:\"{}\"}})'.format(url.unquoted + \"_\" + lang),\n",
    "            'ON CREATE SET articleUrl.url = \"{}\", articleUrl.lang = \"{}\"'.format(url.unquoted, lang),\n",
    "            'SET articleUrl.articleId = {}'.format(article.page_id()),\n",
    "            \"CREATE UNIQUE (articleUrl)-[:RedirectsTo]->(article)\"\n",
    "        ])\n",
    "        \n",
    "        #Create queries for links to the article\n",
    "        for i, (link_href, link_score) in enumerate(links):\n",
    "            #Ensure to escape quotes from the link before query\n",
    "            #var link = pageInfo.links[i].replace(/['\\\\]/g, \"\\\\$&\");\n",
    "            _link_href = escp(QuotedURL(link_href)) #Remove url quotes from the href\n",
    "\n",
    "            neo_query += \"\\n\\n\" + \"\\n\".join([\n",
    "                'MERGE (articleLink{}:Wikiurl{{url_lang: \"{}\"}})'.format(i, _link_href.unquoted+\"_\"+lang),\n",
    "                'ON CREATE SET articleLink{}.url = \"{}\", articleLink{}.lang = \"{}\"'.format(i,_link_href.unquoted,i,lang),\n",
    "                'CREATE UNIQUE (article)-[:LinksTo{{score:{}}}]->(articleLink{})'.format(link_score, i)\n",
    "            ])\n",
    "            \n",
    "        #Create query to create direct connection between articles\n",
    "        if False:\n",
    "            neo_query += \"\\n\"\n",
    "\n",
    "            #Add in/out ConnectsTo relation to this article \n",
    "            neo_query += \"\\n\".join([\n",
    "                \"WITH article MATCH (article)-[:LinksTo]->(:Wikiurl)-[:RedirectsTo]->(targetArticle:Article)\",\n",
    "                \"CREATE UNIQUE (article)-[:ConnectsTo]->(targetArticle)\",\n",
    "                \"WITH article MATCH (targetArticle:Article)-[:LinksTo]->(:Wikiurl)-[:RedirectsTo]->(article)\", \n",
    "                \" CREATE UNIQUE (targetArticle)-[:ConnectsTo]->(article)\"\n",
    "            ])\n",
    "            \n",
    "        neo_query += \"\\nRETURN id(article) as article_id\"\n",
    "\n",
    "        session = self.driver.session()\n",
    "        results = session.run(neo_query)\n",
    "        session.close()\n",
    "        \n",
    "        for record in results:\n",
    "            return record[\"article_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\"\n",
      "test'\n"
     ]
    }
   ],
   "source": [
    "def escp(text):\n",
    "    return text\n",
    "    \"\"\"Function to escape special characters.\"\"\"\n",
    "    new_text = text\n",
    "    new_text = new_text.replace('\"', '\\\\\"') #Two bars because one escape the python code and the other the query str\n",
    "    new_text = new_text.replace(\"'\", \"\\\\'\")\n",
    "    \n",
    "    return new_text\n",
    "    \n",
    "print(escp('test\"'))\n",
    "print(escp(\"test'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w4n = Wiki4Neo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_or_download_pagedata_by_href(href, lang):\n",
    "    \"\"\"\n",
    "    Get data from database, downloading missing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    \n",
    "    #1. Get the wikiarticle data from the db by the href \n",
    "    neo_query = \"\\n\".join([\n",
    "        'MATCH (n:Wikiurl{{url:\"{}\", lang:\"{}\"}})-[:RedirectsTo]->(a:WikiArticle)',\n",
    "        'RETURN a.title as title, a.pageId as pageid, id(a) as id'\n",
    "    ]).format(escp(href.unquoted), lang)\n",
    "    \n",
    "    session = w4n.driver.session()\n",
    "    results = session.run(neo_query)\n",
    "    session.close()\n",
    "    \n",
    "    #print(\"Time of the 1st try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    for r in results:\n",
    "        return r['title'], r['pageid'], r['id']\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #2. If the href doesn't return an article, download this href\n",
    "    #Try to get the article\n",
    "    #If suceed, save the href and try to return the article and create a reference from the href\n",
    "    art = wikipedia.get_article_by_href(href, lang)\n",
    "    \n",
    "    neo_query = \"\\n\".join([\n",
    "        'MERGE (u:Wikiurl{{url_lang:\"{}\"}})'.format(escp(href.unquoted) + \"_\" + lang),\n",
    "        'ON CREATE SET u.url = \"{}\", u.lang = \"{}\", u.articleId = {}'.format(escp(href.unquoted), lang, art.page_id()),\n",
    "        'WITH u MATCH (a:WikiArticle{{pageId:{}}})'.format(art.page_id()),\n",
    "        'CREATE UNIQUE (u)-[:RedirectsTo]->(a)',\n",
    "        'RETURN a.title as title, a.pageId as pageid, id(a) as id'\n",
    "    ])\n",
    "    \n",
    "    session = w4n.driver.session()\n",
    "    results = session.run(neo_query)\n",
    "    session.close()\n",
    "    #print(\"Time of the 2nd try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    for r in results:\n",
    "        return r['title'], r['pageid'], r['id']\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #3. If nothing has been returned till now, so the article doesn't exists, lets register it\n",
    "    links = wikilinks.get_article_links_score(art)\n",
    "    new_art_id = w4n.save_article(art, links, href, lang)\n",
    "    #print(\"Time of the 3rd try to locate the data:{}\".format(time.time() - start_time))\n",
    "    \n",
    "    return art.title(), art.page_id(), new_art_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_langs = [\n",
    "    'de',\n",
    "    'en',\n",
    "    'fr',\n",
    "    'es',\n",
    "    'it',\n",
    "    'nl',\n",
    "    'ja',\n",
    "    'pl',\n",
    "    'ru',\n",
    "    'ceb',\n",
    "    'sv',\n",
    "    'vi',\n",
    "    'war'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "from ThreadPool import ThreadPool\n",
    "\n",
    "def download_and_save_global_articles(href_en):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #global_articles = list()\n",
    "\n",
    "    #href_en = \"MQTT\"\n",
    "    \n",
    "    #1. Download english article and get links scores\n",
    "    #art_en = wikipedia.get_article_by_href(href_en, \"en\")\n",
    "    #links_scores = wikilinks.get_article_links_score(art_en)\n",
    "    #global_articles.append((art_en, links_scores, href_en, \"en\"))\n",
    "    #global_article_title = art_en.title()\n",
    "    #global_article_pageid = art_en.page_id()\n",
    "    \n",
    "    #1.1 Get or download english article data\n",
    "    print(\"Getting english article data...\")\n",
    "    art_en_title, art_en_page_id, art_en_id = get_or_download_pagedata_by_href(href_en, \"en\")\n",
    "    \n",
    "    global_article_title = art_en_title\n",
    "    global_article_pageid = art_en_page_id\n",
    "    \n",
    "    #2. Get global links by english article title\n",
    "    print(\"Getting global links...\")\n",
    "    #global_links = wikipedia.get_article_langlinks(art_en.title())\n",
    "    #Quote title for correct results for cases of \"C++\" etc\n",
    "    global_links = wikipedia.get_article_langlinks(UnquotedURL(art_en_title)) \n",
    "    \n",
    "\n",
    "    #3. Download articles and generate links scores fot the global links\n",
    "    #for art_lang, art_title in global_links:\n",
    "        #art_href = quote(art_title)\n",
    "        #art = wikipedia.get_article_by_href(art_href, art_lang)\n",
    "        #art_links_scores = wikilinks.get_article_links_score(art)\n",
    "        #global_articles.append((art, art_links_scores, art_href, art_lang))\n",
    "        \n",
    "    pool = ThreadPool(10)\n",
    "        \n",
    "    global_pageids = list()\n",
    "    global_pageids.append(art_en_id)\n",
    "    print(\"Getting global links articles data...\") \n",
    "    #3.1 Register hrefs from the global links and get their register ids\n",
    "    \n",
    "    def _get_page_data(i, total, art_title, art_href, art_lang):\n",
    "    \n",
    "        print(\"{} {} - {}/{}\".format(art_title, art_lang, i+1, total))\n",
    "        #try:\n",
    "        art_title, art_page_id, art_id = get_or_download_pagedata_by_href(art_href, art_lang)\n",
    "        global_pageids.append(art_id)\n",
    "        #except:\n",
    "            #print(\"ERROR. Skiping.\")\n",
    "            \n",
    "        print(\"Done {} {} - {}/{}\".format(art_title, art_lang, i+1, total))\n",
    "    \n",
    "    \n",
    "    for i, (art_lang, art_title, art_href) in enumerate(global_links):\n",
    "        #if art_lang not in search_langs:\n",
    "            #continue\n",
    "            \n",
    "        #art_href = quote(art_title) #get href by quoting title\n",
    "        \n",
    "        pool.add_task(_get_page_data, i+1, len(global_links), art_title, art_href, art_lang)\n",
    "        \n",
    "        #print(\"{} {} - {}/{}\".format(art_title, art_lang, i+1, len(global_links)))\n",
    "        #try:\n",
    "            #art_title, art_page_id, art_id = get_or_download_pagedata_by_href(art_href, art_lang)\n",
    "            #global_pageids.append(art_id)\n",
    "        #xcept:\n",
    "            #print(\"ERROR. Skiping.\")\n",
    "        \n",
    "        \n",
    "    pool.wait_completion()\n",
    "    \n",
    "    print(global_pageids)\n",
    "\n",
    "    #4. Save them into the database and get their ids\n",
    "    #global_pageids = list()\n",
    "    #for art in global_articles:\n",
    "        #new_page_ids = w4n.save_article(art[0], art[1], art[2], art[3])\n",
    "        #global_pageids.append(new_page_ids)\n",
    "    \n",
    "    \n",
    "    #5. Points every article to the global article of its kind\n",
    "    print(\"Saving global articles...\")\n",
    "    neo_query = \"\\n\".join([\n",
    "        'MERGE (art:GlobalArticle{{title:\"{}\", en_page_id:{}}})'.format(global_article_title, global_article_pageid),\n",
    "        \"WITH art MATCH (target_art:WikiArticle) WHERE ID(target_art) IN {}\".format(global_pageids),\n",
    "        \"CREATE UNIQUE (art)<-[:DerivesFrom]-(target_art)\"\n",
    "    ])    \n",
    "\n",
    "    session = w4n.driver.session()\n",
    "    session.run(neo_query)\n",
    "    session.close()\n",
    "    print(\"Done ({} seconds.)\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ThreadPool import ThreadPool\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "#from urllib.parse import unquote\n",
    "\n",
    "def download_and_create_langlinks_dict(href_list):\n",
    "    \n",
    "    langlinks_dict = defaultdict(dict)\n",
    "    done = [0]\n",
    "    total = len(href_list)\n",
    "    \n",
    "    def _download_and_save_langlinks(href_en):\n",
    "        for _lang, _title, _href in wikipedia.get_article_langlinks(href_en):\n",
    "           langlinks_dict[_lang][_href.unquoted] = href_en.unquoted\n",
    "        done[0] += 1\n",
    "        clear_output(wait=True)\n",
    "        print(\"Done {}/{}\".format(done[0], total))\n",
    "        \n",
    "    pool = ThreadPool(10)\n",
    "    \n",
    "    for href in href_list:\n",
    "        pool.add_task(_download_and_save_langlinks, href)\n",
    "        \n",
    "    pool.wait_completion()\n",
    "    \n",
    "    return langlinks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download_and_create_langlinks_dict(['C%2b%2b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting english article data...\n",
      "Getting global links...\n",
      "Getting global links articles data...\n",
      "生成对抗网络 zh - 3/2Generative adversarial networks simple - 2/2\n",
      "\n",
      "Done Generative adversarial networks simple - 2/2\n",
      "Done 生成对抗网络 zh - 3/2\n",
      "[17458, 17477, 17474]\n",
      "Saving global articles...\n",
      "Done (4.829276084899902 seconds.)\n"
     ]
    }
   ],
   "source": [
    "download_and_save_global_articles(UnquotedURL(\"Generative adversarial networks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 321/321\n",
      "Done 321/321\n"
     ]
    }
   ],
   "source": [
    "query_title = \"JavaScript\"\n",
    "\n",
    "query = \"\\n\".join(['MATCH (n:GlobalArticle)<-[:DerivesFrom]-(:WikiArticle)-[l:LinksTo]->(u:Wikiurl)',\n",
    "                   'WHERE n.title = \"{}\" RETURN l.score as score,u.url as url, u.lang as lang'\n",
    "                  ]).format(query_title)\n",
    "\n",
    "session = w4n.driver.session()\n",
    "result = session.run(query)\n",
    "session.close()\n",
    "\n",
    "\n",
    "#Get results and fil the lang links list\n",
    "from collections import defaultdict, Counter\n",
    "lang_links = defaultdict(list)\n",
    "for r in result:\n",
    "    lang_links[r['lang']].append([r['url'], r['score']])\n",
    "    \n",
    "#Normalize scores for every lang\n",
    "for lang in lang_links.keys():\n",
    "    score_sum = 0\n",
    "    for url, score in lang_links[lang]:\n",
    "        score_sum += score\n",
    "    for i,(url, score) in enumerate(lang_links[lang]):\n",
    "        lang_links[lang][i].append(score/score_sum)\n",
    "             \n",
    "#Create lang links dict\n",
    "href_list = list()\n",
    "for href_en, _, _ in lang_links['en']:\n",
    "    href_list.append(UnquotedURL(href_en))\n",
    "ll_dict = download_and_create_langlinks_dict(href_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulate everything (assuming urls share href among languages)\n",
    "#Maybe weight things by the number urls they have\n",
    "#And get only things that the english version links to\n",
    "\n",
    "#result_score = Counter()\n",
    "all_langlinks = Counter()\n",
    "all_langlinks_norm = Counter()\n",
    "filtered_langlinks = Counter()\n",
    "filtered_langlinks_norm = Counter()\n",
    "only_english = Counter()\n",
    "\n",
    "search_langs\n",
    "for lang in lang_links.keys():\n",
    "    for url, score, score_norm in lang_links[lang]:\n",
    "        \n",
    "        if lang == \"en\":\n",
    "            global_url = url\n",
    "            only_english[global_url] += score\n",
    "        else:\n",
    "            try:\n",
    "                global_url = ll_dict[lang][url]\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if lang in search_langs:\n",
    "            filtered_langlinks[global_url] += score\n",
    "            filtered_langlinks_norm[global_url] += score_norm\n",
    "        \n",
    "        all_langlinks[global_url] += score\n",
    "        all_langlinks_norm[global_url] += score_norm\n",
    "            \n",
    "        #result_score[global_url] += score_norm        \n",
    "\n",
    "#result_score.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "n_elements = 20\n",
    "\n",
    "list1 = all_langlinks.most_common(n_elements)#sorted(result_score.items(), key=lambda a: a[1], reverse=True)[:n_elements]\n",
    "list2 = all_langlinks_norm.most_common(n_elements)#sorted(simple_link.items(), key=lambda a: a[1], reverse=True)[:n_elements]\n",
    "list3 = filtered_langlinks.most_common(n_elements)#sorted(abstract_links.items(), key=lambda a: a[1], reverse=True)[:n_elements]\n",
    "list4 = filtered_langlinks_norm.most_common(n_elements)#sorted(abstract_links.items(), key=lambda a: a[1], reverse=True)[:n_elements]\n",
    "list5 = only_english.most_common(n_elements)#sorted(abstract_links.items(), key=lambda a: a[1], reverse=True)[:n_elements]\n",
    "\n",
    "joined_list = list(zip(*list1)) + list(zip(*list2)) + list(zip(*list3)) + list(zip(*list4)) + list(zip(*list5))\n",
    "\n",
    "joined_list = list(zip(*joined_list))\n",
    "\n",
    "headers = [\n",
    "    'All langlinks','Score',\n",
    "    'All langlinks_norm','Score',\n",
    "    'Filtered langlinks','Score',\n",
    "    'Filtered langlinks_norm','Score',\n",
    "    'Only english','Score'\n",
    "]\n",
    "\n",
    "folder_name = \"benchmark langlinks scores\"\n",
    "with open(folder_name +\"/\" + query_title + \".txt\", \"w\") as f:\n",
    "    f.write(tabulate(joined_list, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
